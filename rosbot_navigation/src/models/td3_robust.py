"""
æ”¹è¿›çš„TD3æ¨¡å‹ - é€‚é…ROSbotå¯¼èˆªä»»åŠ¡
åŒ…å«é²æ£’æ€§å¢å¼ºå’ŒAMCLä¸ç¡®å®šæ€§å»ºæ¨¡
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Any, Dict, List, Optional, Tuple, Union

from stable_baselines3 import TD3 
from stable_baselines3.common.logger import Logger, configure
from stable_baselines3.common.utils import get_schedule_fn
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

# MLflowå¯¼å…¥
import mlflow
import mlflow.pytorch
# ç›´æ¥ä½¿ç”¨TD3çš„æ ‡å‡†åˆ†å¸ƒï¼Œæ— éœ€é¢å¤–å¯¼å…¥


class RobustTD3Policy(BasePolicy):
    """é²æ£’çš„TD3ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(
        self,
        observation_space,
        action_space,
        lr_schedule,
        net_arch: Optional[List[int]] = None,
        activation_fn: type = nn.ReLU,
        features_extractor_class: type = BaseFeaturesExtractor,
        features_extractor_kwargs: Optional[Dict] = None,
        normalize_images: bool = True,
        optimizer_class: type = optim.Adam,
        optimizer_kwargs: Optional[Dict] = None,
        n_critics: int = 2,
        share_features_extractor: bool = True,
        **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            features_extractor_class,
            features_extractor_kwargs,
            optimizer_class=optimizer_class,
            optimizer_kwargs=optimizer_kwargs,
            squash_output=False,
            normalize_images=normalize_images,
            **kwargs
        )
        
        # ç½‘ç»œæ¶æ„
        self.net_arch = net_arch or [512, 256, 128]
        self.activation_fn = activation_fn
        self.n_critics = n_critics
        self.share_features_extractor = share_features_extractor
        
        # ç‰¹å¾æå–å™¨
        self.features_extractor = features_extractor_class(
            self.observation_space, **features_extractor_kwargs
        )
        
        # é²æ£’æ€§å¢å¼ºå±‚
        self.dropout = nn.Dropout(0.1)
        self.layer_norm = nn.LayerNorm(self.features_extractor.features_dim)
        
        # ç­–ç•¥ç½‘ç»œ
        self.actor = self._build_actor()
        
        # Qç½‘ç»œï¼ˆåŒQç½‘ç»œï¼‰
        self.critics = nn.ModuleList([
            self._build_critic() for _ in range(n_critics)
        ])
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer_class = optimizer_class
        self.optimizer_kwargs = optimizer_kwargs or {}
        self.lr_schedule = lr_schedule
        
        self.actor_optimizer = self.optimizer_class(
            self.actor.parameters(), 
            lr=self.lr_schedule(1),
            **self.optimizer_kwargs
        )
        
        self.critic_optimizer = self.optimizer_class(
            self.critics.parameters(),
            lr=self.lr_schedule(1),
            **self.optimizer_kwargs
        )
        
    def _build_actor(self) -> nn.Module:
        """æ„å»ºç­–ç•¥ç½‘ç»œ"""
        layers = []
        
        layers.append(nn.Linear(self.features_extractor.features_dim, self.net_arch[0]))
        layers.append(self.activation_fn())
        layers.append(nn.Dropout(0.1))
        
        for i in range(len(self.net_arch) - 1):
            layers.append(nn.Linear(self.net_arch[i], self.net_arch[i+1]))
            layers.append(self.activation_fn())
            layers.append(nn.LayerNorm(self.net_arch[i+1]))
            layers.append(nn.Dropout(0.1))
        
        # è¾“å‡ºå±‚ - ç¡®å®šæ€§å’Œå™ªå£°åˆ†æ”¯
        layers.append(nn.Linear(self.net_arch[-1], self.net_arch[-1]))
        
        # ç­–ç•¥å¤´å’Œå™ªå£°å¤´ - å¢å¼ºé•¿æœŸè§„åˆ’èƒ½åŠ›
        # ä½¿ç”¨æ›´å¤§çš„ä¸­é—´å±‚æ¥å¤„ç†å¤šä¸ªåŠ¨ä½œçš„è¾“å‡º
        self.policy_head = nn.Sequential(
            nn.Linear(self.net_arch[-1], self.net_arch[-1] * 2),
            self.activation_fn(),
            nn.LayerNorm(self.net_arch[-1] * 2),
            nn.Linear(self.net_arch[-1] * 2, self.action_space.shape[0])
        )
        
        self.noise_head = nn.Sequential(
            nn.Linear(self.net_arch[-1], self.net_arch[-1] * 2),
            self.activation_fn(),
            nn.LayerNorm(self.net_arch[-1] * 2),
            nn.Linear(self.net_arch[-1] * 2, self.action_space.shape[0])
        )
        
        return nn.Sequential(*layers)
    
    def _build_critic(self) -> nn.Module:
        """æ„å»ºQç½‘ç»œ"""
        layers = []
        
        # è¾“å…¥ï¼šè§‚æµ‹ + åŠ¨ä½œ
        input_dim = self.features_extractor.features_dim + self.action_space.shape[0]
        
        layers.append(nn.Linear(input_dim, self.net_arch[0]))
        layers.append(self.activation_fn())
        layers.append(nn.Dropout(0.1))
        
        for i in range(len(self.net_arch) - 1):
            layers.append(nn.Linear(self.net_arch[i], self.net_arch[i+1]))
            layers.append(self.activation_fn())
            layers.append(nn.LayerNorm(self.net_arch[i+1]))
            layers.append(nn.Dropout(0.1))
        
        # Qå€¼è¾“å‡º
        layers.append(nn.Linear(self.net_arch[-1], 1))
        
        return nn.Sequential(*layers)
    
    def forward(self, obs: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - å¢å¼ºé•¿æœŸè§„åˆ’èƒ½åŠ›
        
        è¾“å‡º10ç»´åŠ¨ä½œç©ºé—´ï¼ŒåŒ…å«5ä¸ªè¿ç»­çš„åŠ¨ä½œï¼Œæ¯ä¸ªåŠ¨ä½œ2ç»´[çº¿é€Ÿåº¦, è§’é€Ÿåº¦]
        """
        # ç‰¹å¾æå–
        features = self.extract_features(obs)
        
        # ç­–ç•¥ç½‘ç»œ
        core_features = self.actor(features)
        
        # ä¸»ç­–ç•¥è¾“å‡º
        mean_actions = self.policy_head(core_features)
        
        # å°†åŠ¨ä½œé‡å¡‘ä¸º(batch_size, 5, 2)ä»¥ä¾¿äºå¤„ç†æ—¶åºä¾èµ–å…³ç³»
        batch_size = mean_actions.shape[0]
        mean_actions_reshaped = mean_actions.view(batch_size, 5, 2)
        
        # åº”ç”¨æ—¶åºå¹³æ»‘åŒ–ï¼Œç¡®ä¿ç›¸é‚»åŠ¨ä½œä¹‹é—´çš„è¿ç»­æ€§
        # å¯¹äºç¬¬ä¸€ä¸ªåŠ¨ä½œä¿æŒåŸæ ·ï¼Œåç»­åŠ¨ä½œä¸å‰ä¸€ä¸ªåŠ¨ä½œä¿æŒä¸€å®šçš„ç›¸å…³æ€§
        for i in range(1, 5):
            # åº”ç”¨å¹³æ»‘å› å­ï¼Œä½¿å½“å‰åŠ¨ä½œéƒ¨åˆ†ä¾èµ–äºå‰ä¸€ä¸ªåŠ¨ä½œ
            smoothing_factor = 0.3  # å¯è°ƒæ•´çš„å¹³æ»‘å› å­
            mean_actions_reshaped[:, i] = mean_actions_reshaped[:, i] * (1 - smoothing_factor) + \
                                        mean_actions_reshaped[:, i-1] * smoothing_factor
        
        # é‡æ–°å±•å¹³ä¸º10ç»´åŠ¨ä½œ
        mean_actions = mean_actions_reshaped.reshape(batch_size, -1)
        
        if deterministic:
            actions = torch.tanh(mean_actions)
        else:
            # å™ªå£°è¾“å‡º
            noise_std = torch.sigmoid(self.noise_head(core_features))
            
            # å°†å™ªå£°é‡å¡‘ä¸º(batch_size, 5, 2)ä»¥åº”ç”¨ä¸åŒçš„å™ªå£°ç­–ç•¥
            noise_std_reshaped = noise_std.view(batch_size, 5, 2)
            
            # è¿œæœŸåŠ¨ä½œçš„å™ªå£°é€æ¸å¢å¤§ï¼Œè¡¨ç¤ºé•¿æœŸä¸ç¡®å®šæ€§å¢åŠ 
            for i in range(1, 5):
                # æ¯ä¸ªæ—¶é—´æ­¥å™ªå£°ç³»æ•°å¢åŠ 
                noise_factor = 1.0 + i * 0.15  # æ¯ä¸ªæ—¶é—´æ­¥å¢åŠ 15%çš„å™ªå£°
                noise_std_reshaped[:, i] = noise_std_reshaped[:, i] * noise_factor
            
            # é‡æ–°å±•å¹³
            noise_std = noise_std_reshaped.reshape(batch_size, -1)
            
            # åº”ç”¨å™ªå£°
            noise = torch.randn_like(mean_actions) * noise_std * 0.1
            actions = torch.tanh(mean_actions + noise)
        
        return actions, mean_actions
    
    def extract_features(self, obs: torch.Tensor) -> torch.Tensor:
        """ç‰¹å¾æå–ï¼ŒåŒ…å«ä¸ç¡®å®šæ€§å¤„ç†"""
        base_features = self.features_extractor(obs)
        
        # å¦‚æœè¾“å…¥åŒ…å«ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼Œè¿›è¡Œå¤„ç†
        if obs.shape[-1] > 40:  # 42ç»´çŠ¶æ€
            # æå–ä¸ç¡®å®šæ€§ç›¸å…³çš„ç‰¹å¾
            uncertainty_features = obs[:, -4:]  # åé¢å‡ ç»´å¯èƒ½åŒ…å«ä¸ç¡®å®šæ€§
            uncertainty_weight = torch.sigmoid(uncertainty_features).mean(dim=-1, keepdim=True)
            
            # æ ¹æ®ä¸ç¡®å®šæ€§è°ƒæ•´ç‰¹å¾æƒé‡
            base_features = base_features * (1 - uncertainty_weight * 0.2)
        
        # åº”ç”¨å½’ä¸€åŒ–å’Œdropout
        base_features = self.layer_norm(base_features)
        base_features = self.dropout(base_features)
        
        return base_features
    
    def q_value_forward(self, obs: torch.Tensor, actions: torch.Tensor) -> List[torch.Tensor]:
        """Qå€¼ç½‘ç»œå‰å‘ä¼ æ’­"""
        features = self.extract_features(obs)
        
        # è¿æ¥è§‚æµ‹å’ŒåŠ¨ä½œ
        q_input = torch.cat([features, actions], dim=1)
        
        # è®¡ç®—å¤šä¸ªQå€¼ï¼ˆåŒQç½‘ç»œï¼‰
        q_values = []
        for critic in self.critics:
            q_value = critic(q_input)
            q_values.append(q_value)
        
        return q_values


class RosbotFeaturesExtractor(BaseFeaturesExtractor):
    """ROSbotä¸“ç”¨çš„ç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space, features_dim: int = 512):
        super().__init__(observation_space, features_dim)

        # ç‰¹å¾æå–ç½‘ç»œ
        # æ³¨æ„ï¼šfeature_fusion è¾“å‡ºä¸º 128 ç»´ï¼Œå› æ­¤æ­¤å¤„æ”¹ä¸ºä» 128 è¾“å…¥å¼€å§‹
        self.net = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.1),
            nn.Linear(128, features_dim),
            nn.ReLU(),
            nn.LayerNorm(features_dim)
        )
        
        # ç‰¹æ®Šç‰¹å¾å¤„ç†å±‚ï¼ˆç¼©å°è§„æ¨¡ï¼‰
        self.lidar_processor = nn.Sequential(
            nn.Linear(20, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        self.amcl_processor = nn.Sequential(
            nn.Linear(12, 32),  # AMCLç‰¹å¾
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        self.navigation_processor = nn.Sequential(
            nn.Linear(10, 32),   # å¯¼èˆªä¿¡æ¯
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # ç‰¹å¾èåˆå±‚ï¼ˆç¼©å°è§„æ¨¡ï¼‰
        self.feature_fusion = nn.Sequential(
            nn.Linear(42, 128),
            nn.ReLU(),
            nn.LayerNorm(128)
        )
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """ç‰¹å¾æå–å‰å‘ä¼ æ’­"""
        batch_size = observations.shape[0]
        
        # æå–å„éƒ¨åˆ†ç‰¹å¾
        #lidar_features = self.lidar_processor(observations[:, 0:20])
        #pose_features = self.amcl_processor(observations[:, 20:32])
        #nav_features = self.navigation_processor(observations[:, 32:42])
        
        # å…¶ä»–ç‰¹å¾ç›´æ¥å¤„ç†
        #other_features = observations[:, 20:42]  # å…¶ä»–çŠ¶æ€ä¿¡æ¯
        
        # ç‰¹å¾èåˆ
        # combined_features = torch.cat([
        #     lidar_features,    # 16ç»´
        #     pose_features,     # 16ç»´ 
        #     nav_features      # 16ç»´
        #     # other_features     
        # ], dim=1)  # æ€»å…±48ç»´
        # fused_features = self.feature_fusion(combined_features)
        # ä¸è¿›è¡Œç‰¹å¾æå–ï¼Œç›´æ¥å°†42ç»´è§‚æµ‹è¾“å…¥èåˆå±‚
        fused_features = self.feature_fusion(observations)
        
        # æœ€ç»ˆç‰¹å¾æå–
        final_features = self.net(fused_features)
        
        return final_features


class AdaptiveRewardModel(nn.Module):
    """è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ - æ ¹æ®è´§ç‰©ç±»å‹è°ƒæ•´"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super().__init__()
        
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3)  # 3ç§è´§ç‰©ç±»å‹çš„å¥–åŠ±åˆ†é‡
        )
    
    def forward(self, state: torch.Tensor, cargo_type: str) -> torch.Tensor:
        """è®¡ç®—è‡ªé€‚åº”å¥–åŠ±åˆ†é‡"""
        base_rewards = self.net(state)
        
        # æ ¹æ®è´§ç‰©ç±»å‹é€‰æ‹©å¥–åŠ±åˆ†é‡
        if cargo_type == 'fragile':
            return base_rewards[:, 0:1]
        elif cargo_type == 'dangerous':
            return base_rewards[:, 1:2]
        else:
            return base_rewards[:, 2:3]


class ImprovedTD3(TD3):
    """æ”¹è¿›çš„TD3ç®—æ³• - é€‚é…ROSbotå¯¼èˆª"""
    
    def __init__(
        self,
        policy: Union[str, type[BasePolicy]] = RobustTD3Policy,
        env: Optional[Any] = None,
        learning_rate: Union[float, Any] = 5e-4,
        buffer_size: int = 500000,
        learning_starts: int = 1000,
        batch_size: int = 256,
        tau: float = 0.005,
        gamma: float = 0.98,
        train_freq: Union[int, Any] = 1,
        gradient_steps: int = 1,
        action_noise: Optional[Any] = None,
        replay_buffer_class = None,
        replay_buffer_kwargs: Optional[Dict] = None,
        optimize_memory_usage: bool = False,
        policy_delay: int = 2,
        target_policy_noise: float = 0.3,
        target_noise_clip: float = 0.5,
        policy_kwargs: Optional[Dict] = None,
        verbose: int = 0,
        seed: Optional[int] = None,
        device: Union[torch.device, str] = "auto",
        _init_setup_model: bool = True,
        **kwargs
    ):
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self._default_logger = configure(None, ["stdout"])
        # æ”¹è¿›çš„é»˜è®¤å‚æ•°
        policy_kwargs = policy_kwargs or {}
        policy_kwargs.update({
            'net_arch': [512, 256, 128],
            'features_extractor_class': RosbotFeaturesExtractor,
            'features_extractor_kwargs': {'features_dim': 512}
        })
        
        super().__init__(
            policy=policy,
            env=env,
            learning_rate=learning_rate,
            buffer_size=buffer_size,
            learning_starts=learning_starts,
            batch_size=batch_size,
            tau=tau,
            gamma=gamma,
            train_freq=train_freq,
            gradient_steps=gradient_steps,
            action_noise=action_noise,
            replay_buffer_class=replay_buffer_class,
            replay_buffer_kwargs=replay_buffer_kwargs,
            optimize_memory_usage=optimize_memory_usage,
            policy_delay=policy_delay,
            target_policy_noise=target_policy_noise,
            target_noise_clip=target_noise_clip,
            policy_kwargs=policy_kwargs,
            verbose=verbose,
            seed=seed,
            device=device,
            _init_setup_model=_init_setup_model,
            **kwargs
        )
    
    def get_model_architecture_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ¶æ„çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºè®°å½•åˆ°MLflow"""
        # è·å–ç­–ç•¥ç½‘ç»œä¿¡æ¯
        if not hasattr(self, "policy") or self.policy is None:
            return {"error": "Policy not initialized"}
        
        # åŸºæœ¬æ¶æ„ä¿¡æ¯
        architecture_info = {
            "model_type": "ImprovedTD3",
            "policy_type": self.policy.__class__.__name__,
            "policy_kwargs": self.policy_kwargs,
        }
        
        # å¦‚æœç­–ç•¥å·²åˆå§‹åŒ–ï¼Œè·å–æ›´è¯¦ç»†çš„ç½‘ç»œç»“æ„
        if hasattr(self.policy, "actor") and self.policy.actor is not None:
            # è·å–ç­–ç•¥ç½‘ç»œç»“æ„
            actor_layers = []
            for name, module in self.policy.actor.named_children():
                actor_layers.append(f"{name}: {module.__class__.__name__}")
                if hasattr(module, "in_features") and hasattr(module, "out_features"):
                    actor_layers[-1] += f" ({module.in_features} -> {module.out_features})"
            
            # è·å–ç­–ç•¥å¤´ç»“æ„
            policy_head_layers = []
            for name, module in self.policy.policy_head.named_children():
                policy_head_layers.append(f"{name}: {module.__class__.__name__}")
                if hasattr(module, "in_features") and hasattr(module, "out_features"):
                    policy_head_layers[-1] += f" ({module.in_features} -> {module.out_features})"
            
            # è·å–å™ªå£°å¤´ç»“æ„
            noise_head_layers = []
            for name, module in self.policy.noise_head.named_children():
                noise_head_layers.append(f"{name}: {module.__class__.__name__}")
                if hasattr(module, "in_features") and hasattr(module, "out_features"):
                    noise_head_layers[-1] += f" ({module.in_features} -> {module.out_features})"
            
            # è·å–ç‰¹å¾æå–å™¨ç»“æ„
            features_extractor_layers = []
            if hasattr(self.policy, "features_extractor") and self.policy.features_extractor is not None:
                for name, module in self.policy.features_extractor.named_children():
                    features_extractor_layers.append(f"{name}: {module.__class__.__name__}")
                    # å¦‚æœæ˜¯Sequentialï¼Œè¿›ä¸€æ­¥è·å–å…¶å­æ¨¡å—
                    if isinstance(module, nn.Sequential):
                        for i, submodule in enumerate(module):
                            features_extractor_layers.append(f"  {i}: {submodule.__class__.__name__}")
                            if hasattr(submodule, "in_features") and hasattr(submodule, "out_features"):
                                features_extractor_layers[-1] += f" ({submodule.in_features} -> {submodule.out_features})"
            
            # è·å–è¯„è®ºå®¶ç½‘ç»œç»“æ„
            critic_layers = []
            if hasattr(self.policy, "critics") and len(self.policy.critics) > 0:
                for i, critic in enumerate(self.policy.critics):
                    critic_layers.append(f"Critic {i}:")
                    for j, module in enumerate(critic):
                        critic_layers.append(f"  {j}: {module.__class__.__name__}")
                        if hasattr(module, "in_features") and hasattr(module, "out_features"):
                            critic_layers[-1] += f" ({module.in_features} -> {module.out_features})"
            
            # è®¡ç®—å‚æ•°æ€»é‡
            total_params = sum(p.numel() for p in self.policy.parameters() if p.requires_grad)
            
            # æ›´æ–°æ¶æ„ä¿¡æ¯
            architecture_info.update({
                "actor_layers": actor_layers,
                "policy_head_layers": policy_head_layers,
                "noise_head_layers": noise_head_layers,
                "features_extractor_layers": features_extractor_layers,
                "critic_layers": critic_layers,
                "total_trainable_parameters": total_params,
                "observation_space": str(self.observation_space),
                "action_space": str(self.action_space),
                "learning_rate": self.learning_rate,
                "buffer_size": self.buffer_size,
                "batch_size": self.batch_size,
                "tau": self.tau,
                "gamma": self.gamma,
                "policy_delay": self.policy_delay,
                "target_policy_noise": self.target_policy_noise,
                "target_noise_clip": self.target_noise_clip
            })
        
        return architecture_info
    
    def log_model_architecture_to_mlflow(self):
        """å°†æ¨¡å‹æ¶æ„è®°å½•åˆ°MLflow"""
        if not mlflow.active_run():
            print("æ²¡æœ‰æ´»è·ƒçš„MLflowè¿è¡Œï¼Œæ— æ³•è®°å½•æ¨¡å‹æ¶æ„")
            return
        
        try:
            # è·å–æ¨¡å‹æ¶æ„ä¿¡æ¯
            architecture_info = self.get_model_architecture_info()
            
            # å°†æ¶æ„ä¿¡æ¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¾¿äºMLflowè®°å½•
            architecture_str = "\n".join([
                "# æ¨¡å‹æ¶æ„è¯¦æƒ…",
                f"## åŸºæœ¬ä¿¡æ¯",
                f"- æ¨¡å‹ç±»å‹: {architecture_info['model_type']}",
                f"- ç­–ç•¥ç±»å‹: {architecture_info['policy_type']}",
                f"- å¯è®­ç»ƒå‚æ•°æ€»é‡: {architecture_info.get('total_trainable_parameters', 'N/A')}",
                f"- è§‚æµ‹ç©ºé—´: {architecture_info.get('observation_space', 'N/A')}",
                f"- åŠ¨ä½œç©ºé—´: {architecture_info.get('action_space', 'N/A')}",
                
                f"\n## è¶…å‚æ•°",
                f"- å­¦ä¹ ç‡: {architecture_info.get('learning_rate', 'N/A')}",
                f"- ç¼“å†²åŒºå¤§å°: {architecture_info.get('buffer_size', 'N/A')}",
                f"- æ‰¹æ¬¡å¤§å°: {architecture_info.get('batch_size', 'N/A')}",
                f"- Tau: {architecture_info.get('tau', 'N/A')}",
                f"- Gamma: {architecture_info.get('gamma', 'N/A')}",
                f"- ç­–ç•¥å»¶è¿Ÿ: {architecture_info.get('policy_delay', 'N/A')}",
                f"- ç›®æ ‡ç­–ç•¥å™ªå£°: {architecture_info.get('target_policy_noise', 'N/A')}",
                f"- ç›®æ ‡å™ªå£°è£å‰ª: {architecture_info.get('target_noise_clip', 'N/A')}",
                
                f"\n## ç‰¹å¾æå–å™¨",
                *[f"- {layer}" for layer in architecture_info.get('features_extractor_layers', ['N/A'])],
                
                f"\n## Actorç½‘ç»œ",
                *[f"- {layer}" for layer in architecture_info.get('actor_layers', ['N/A'])],
                
                f"\n### ç­–ç•¥å¤´",
                *[f"- {layer}" for layer in architecture_info.get('policy_head_layers', ['N/A'])],
                
                f"\n### å™ªå£°å¤´",
                *[f"- {layer}" for layer in architecture_info.get('noise_head_layers', ['N/A'])],
                
                f"\n## Criticç½‘ç»œ",
                *[f"- {layer}" for layer in architecture_info.get('critic_layers', ['N/A'])],
            ])
            
            # è®°å½•æ¶æ„ä¿¡æ¯åˆ°MLflow
            mlflow.log_text(architecture_str, "model_architecture.md")
            
            # è®°å½•ç­–ç•¥å‚æ•°
            mlflow.log_dict(architecture_info, "model_architecture.json")
            
            # è®°å½•PyTorchæ¨¡å‹ç»“æ„å›¾ (å¯é€‰ï¼Œéœ€è¦graphvizæ”¯æŒ)
            try:
                # å°è¯•ä½¿ç”¨torchvizè®°å½•æ¨¡å‹ç»“æ„å›¾
                import torch
                from torchviz import make_dot
                
                # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥
                dummy_input = torch.zeros((1, *self.observation_space.shape), 
                                         dtype=torch.float32, 
                                         device=self.device)
                
                # è·å–æ¨¡å‹è¾“å‡º
                with torch.no_grad():
                    actions, _ = self.policy(dummy_input)
                
                # åˆ›å»ºè®¡ç®—å›¾
                dot = make_dot(actions, params=dict(self.policy.named_parameters()))
                
                # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
                import tempfile
                import os
                
                with tempfile.TemporaryDirectory() as tmpdirname:
                    dot_path = os.path.join(tmpdirname, "model_graph")
                    dot.render(dot_path, format="png")
                    mlflow.log_artifact(f"{dot_path}.png", "model_architecture")
            except ImportError:
                print("torchvizæœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹ç»“æ„å›¾è®°å½•")
            except Exception as e:
                print(f"è®°å½•æ¨¡å‹ç»“æ„å›¾å¤±è´¥: {e}")
            
            print("âœ… æ¨¡å‹æ¶æ„å·²æˆåŠŸè®°å½•åˆ°MLflow")
            
        except Exception as e:
            print(f"è®°å½•æ¨¡å‹æ¶æ„åˆ°MLflowå¤±è´¥: {e}")
    
    def learn(
        self,
        total_timesteps: int,
        callback=None,
        log_interval: int = 4,
        tb_log_name: str = "TD3",
        reset_num_timesteps: bool = True,
        progress_bar: bool = True,
        use_mlflow: bool = True,
        mlflow_run_name: str = None,
        mlflow_experiment_name: str = None
    ):
        """è®­ç»ƒå­¦ä¹  - å¢å¼ºç‰ˆæœ¬"""
        print(f"å¼€å§‹ROSbotå¯¼èˆªè®­ç»ƒ - TD3ç®—æ³•")
        print(f"æ€»æ­¥æ•°: {total_timesteps}")
        print(f"çŠ¶æ€ç©ºé—´: 42ç»´ (LiDAR+AMCL+å¯¼èˆªä¿¡æ¯)")
        print(f"åŠ¨ä½œç©ºé—´: 2ç»´è¿ç»­ [çº¿é€Ÿåº¦, è§’é€Ÿåº¦]")
        print(f"AMCLç²’å­æ•°: 800")
        print(f"âœ… å·²å¯ç”¨æ¢¯åº¦è£å‰ª (max_norm=0.5)")
        
        # ç¡®ä¿loggerå¯ç”¨
        if self._logger is None:
            self._logger = configure(None, ["stdout"])
        
        # å¦‚æœå¯ç”¨MLflowä¸”æ²¡æœ‰æ´»è·ƒçš„è¿è¡Œï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„è¿è¡Œ
        if use_mlflow and mlflow.active_run() is None:
            try:
                # è®¾ç½®å®éªŒåç§°ï¼ˆå¦‚æœæä¾›ï¼‰
                if mlflow_experiment_name:
                    mlflow.set_experiment(mlflow_experiment_name)
                
                # å¼€å§‹ä¸€ä¸ªæ–°çš„MLflowè¿è¡Œ
                run_name = mlflow_run_name or f"td3_{tb_log_name}_{total_timesteps}"
                mlflow.start_run(run_name=run_name)
                
                # è®°å½•æ¨¡å‹è¶…å‚æ•°
                mlflow.log_params({
                    "learning_rate": self.learning_rate,
                    "buffer_size": self.buffer_size,
                    "learning_starts": self.learning_starts,
                    "batch_size": self.batch_size,
                    "tau": self.tau,
                    "gamma": self.gamma,
                    "train_freq": self.train_freq,
                    "gradient_steps": self.gradient_steps,
                    "policy_delay": self.policy_delay,
                    "target_policy_noise": self.target_policy_noise,
                    "target_noise_clip": self.target_noise_clip,
                    "total_timesteps": total_timesteps
                })
                
                # è®°å½•æ¨¡å‹æ¶æ„ä¿¡æ¯
                self.log_model_architecture_to_mlflow()
                
                print(f"MLflowè·Ÿè¸ªå·²åˆå§‹åŒ–: {mlflow.get_artifact_uri()}")
            except Exception as e:
                print(f"MLflowåˆå§‹åŒ–å¤±è´¥: {e}")
        
        return super().learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=log_interval,
            tb_log_name=tb_log_name,
            reset_num_timesteps=reset_num_timesteps,
            progress_bar=progress_bar
        )
    
    def _update_policy(self, gradient_steps: int, batch_size: int = 64):
        """é‡å†™ç­–ç•¥æ›´æ–°ä»¥æ·»åŠ æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ ç‡è¡°å‡"""
        # ç¡®ä¿æ—¥å¿—è®°å½•å™¨å·²åˆå§‹åŒ–
        if self._logger is None:
            self._logger = configure(None, ["stdout"])
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•å‰æ”¶é›†æŒ‡æ ‡
        policy_losses = []
        critic_losses = []
        
        # æ‰§è¡Œç­–ç•¥æ›´æ–°
        for _ in range(gradient_steps):
            # æ›´æ–°critic
            critic_loss = self._update_critic(batch_size)
            if critic_loss is not None:
                critic_losses.append(critic_loss.item())
            
            # å»¶è¿Ÿæ›´æ–°actor
            if self._n_updates % self.policy_delay == 0:
                # è®¡ç®—actor loss
                actor_loss = self._update_actor(batch_size)
                if actor_loss is not None:
                    policy_losses.append(actor_loss.item())
                
                # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                self._update_target_networks()
            
            self._n_updates += 1
        
        # è®°å½•å¹³å‡ç­–ç•¥æŸå¤±
        if len(policy_losses) > 0:
            policy_loss_mean = np.mean(policy_losses)
            self.logger.record("train/policy_loss", policy_loss_mean)
            
            # è®°å½•åˆ°MLflowï¼ˆå¦‚æœæœ‰æ´»è·ƒçš„è¿è¡Œï¼‰
            if mlflow.active_run() is not None:
                try:
                    mlflow.log_metric("policy_loss", policy_loss_mean, step=self._n_updates)
                except Exception as e:
                    print(f"MLflowè®°å½•ç­–ç•¥æŸå¤±å¤±è´¥: {e}")
        
        # è®°å½•å¹³å‡criticæŸå¤±
        if len(critic_losses) > 0:
            critic_loss_mean = np.mean(critic_losses)
            self.logger.record("train/critic_loss", critic_loss_mean)
            
            # è®°å½•åˆ°MLflowï¼ˆå¦‚æœæœ‰æ´»è·ƒçš„è¿è¡Œï¼‰
            if mlflow.active_run() is not None:
                try:
                    mlflow.log_metric("critic_loss", critic_loss_mean, step=self._n_updates)
                except Exception as e:
                    print(f"MLflowè®°å½•criticæŸå¤±å¤±è´¥: {e}")
        
        # å­¦ä¹ ç‡è¡°å‡
        if hasattr(self.policy.optimizer, "param_groups"):
            current_lr = self.policy.optimizer.param_groups[0]["lr"]
            self.logger.record("train/learning_rate", current_lr)
            
            # è®°å½•åˆ°MLflowï¼ˆå¦‚æœæœ‰æ´»è·ƒçš„è¿è¡Œï¼‰
            if mlflow.active_run() is not None:
                try:
                    mlflow.log_metric("learning_rate", current_lr, step=self._n_updates)
                except Exception as e:
                    print(f"MLflowè®°å½•å­¦ä¹ ç‡å¤±è´¥: {e}")
        
        # æ·»åŠ æ¢¯åº¦è£å‰ª
        if hasattr(self.policy, 'actor'):
            torch.nn.utils.clip_grad_norm_(self.policy.actor.parameters(), max_norm=0.5)
        if hasattr(self.policy, 'critics'):
            for critic in self.policy.critics:
                torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=0.5)
        
        # å­¦ä¹ ç‡è¡°å‡ (è®­ç»ƒè¶…è¿‡5000æ­¥åå¼€å§‹) - æš‚æ—¶ç¦ç”¨
        if self.num_timesteps > 5000 and self.num_timesteps % 5000 == 0:
            decay_factor = max(0.1, 1.0 - (self.num_timesteps - 5000) / 45000)  # 5k-50kçº¿æ€§è¡°å‡åˆ°10%
            current_lr = 3e-4 * decay_factor
            print(f"ğŸ“‰ Step {self.num_timesteps}: å­¦ä¹ ç‡è¡°å‡è®¡åˆ’ - ç›®æ ‡LR: {current_lr:.2e} (è¡°å‡å› å­: {decay_factor:.3f})")
            print(f"   æ³¨æ„: åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´æš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡è®­ç»ƒ")
        
        return result
    
    def _setup_model(self) -> None:
        """æ¨¡å‹è®¾ç½®å¢å¼º"""
        super()._setup_model()
        
        # AMCLä¸ç¡®å®šæ€§æ—¥å¿—è®°å½• - AMCLå·²åœç”¨ï¼Œæš‚æ—¶æ³¨é‡Š
        # self.logger.record("amcl/effective_particles", -1)  # å ä½ç¬¦
        # self.logger.record("amcl/position_uncertainty", -1)
        # self.logger.record("amcl/convergence_score", -1)
    
    def _excluded_save_params(self) -> List[str]:
        """æ’é™¤ä¿å­˜çš„å‚æ•°"""
        excluded = super()._excluded_save_params()
        return excluded
        
    @property
    def logger(self) -> Logger:
        """ç¡®ä¿å§‹ç»ˆæœ‰å¯ç”¨çš„æ—¥å¿—è®°å½•å™¨"""
        if not hasattr(self, "_logger") or self._logger is None:
            return self._default_logger
        return self._logger