

### learner_VerticalTest_end_150000_20251001_144806 
#### 修改
1. 增大了角度的占比，给角度奖励增加了距离加成，距离越近，角度奖励越大，但是反比例造成远端奖励衰减
2. 使用左右轮速度做转圈检测，基于角度奖励转圈衰减
3. time_k 0.05 降低时间惩罚
4. 将到达奖励从 500 提升到 2000
5. 放宽角度奖励条件：移除“front_clear 且 夹角<0.6π”的限制，改为无条件给予角度奖励；当 front_clear 为真时再放大 1.5 倍
6. 卡住检测放宽：stuck_termination_threshold 20→40，stuck_position_threshold 0.1→0.05（降低误判）
#### 结果
6. 空白房间成功率有效提升到40左右
7. 训练依旧不稳定，远端奖励衰减导致角度在较远处无法获得奖励
8. 似乎依旧存在刷奖励情况，成功率最高的时候不是奖励最高的时候，


### learner_VerticalTest_end_150000_20251001_155949
#### 问题
- 朝终点方向走程度不够，成功率约40%
- 抑制“乱走刷奖励”

#### 修改
1. 再一次增大角度奖励占比，距离增益系数改成正二次型，最开始大，中间小，最后大
   - 修改“方向性移动奖励 directional_movement”：修改距离加成反比函数为二次型((distance-6)^2*0.2+1)；
   - 修改为distance_change,现在距离减小的时候角度有惩罚
2. 增大靠墙惩罚开始距离到1m，增大靠近墙的惩罚系数，从 3→5；不再给予靠近终点衰减；
3. 减小基础距离奖励系数， 0.5
4. 朝目标方向移动奖励系数增大，50
5. 增大 front_clear系数，增大奖励，1→3
6. 统一了旋转步数计算方式，使用两轮差速计算

#### 结果
1. 似乎有一些矫枉过正，训练还是不太稳定
2. 成功率可以稳定40左右

10月2日

#### 修改
1. 修复了训练流程不正确的问题。现在先使用单仿真进行训练。
2. 修改了一些函数，把vertical的目的地位置获取函数设置为到三个目标点
3. 在复杂环境下成功率较低。主要问题是最后无法到达目标点，一直在周围打转。
4. 模型会往上方钻空子，把上面的空子堵了。
5. 角速度较大时给予惩罚
6. 提高完成奖励，2000→5000。可能由于目标附近转圈回报太大导致不结束。
7. 减小到达阈值，0.2→0.1

### Release 4.5-4.6
#### 表现：
测试了使用local_map,使用twist，都不理想。
修改了诸多奖励函数参数，使用渐进的随机障碍物，效果都不好。
依旧是老问题，无法到达目标点。接近率也不够理想，只有0.5左右，实际经过目标点附近的可能到0.7，但是成功率只有百分之十
#### 思考
1. 是否进一步返璞归真？简化奖励函数。actor和critic也没能很好的拟合上，十分不稳定。
2. 是否在奖励函数里面使用了难以通过模型学习到的奖励？角度奖励现在使用的是移动夹角，不是角速度夹角，使用左右轮速度推到角速度再推到夹角可能对模型来说有困难。


## Release 4.6.1~4.6.3
### Release 4.6.1
把模型观测到的偏差角度改为移动方向夹角 不动奖励函数 4.6.1 Env_test4.6.1_env1

两次运行都证明，接近率和完成率在前期有显著的提升，但是训练久了以后依旧会往回走，不稳定导致下降。
模型似乎往回走不结束会有更大的累计回报。critic会逐渐变大。

### Release 4.6.2 
照开源奖励函数，对现有奖励函数进行简化，同时保留了4.6.1使用移动方向夹角
无效。

### Release 4.6.3
#### Release 4.6.3.0
保留了4.6.1使用移动方向夹角，使用原先奖励。
4.6.1以及之前的问题都是，车辆很爱乱逛和往回走。

该版本设置 当移动距离小于0，基础距离奖励*0.5。
同时删去给予步数的角度奖惩励衰减，给予角度双重符号约束，即只要角度和距离有一个负数就整个是负数；同时增大步数惩罚，系数0.02-0.5。
可以计划写成距离二次型，距离越远步数惩罚越大。
可以计划靠近也给很多奖励，这个范围从1可以扩大到没有障碍物，直线可以到达的区域。只要能到达没有障碍物的区域，最后到达目标点可以使用手动对齐，手动设置角度和速度让车辆过去。这个在下一个版本给出。

表现：
四个障碍物情况下 六万步 实现高达0.99的成功率 真无敌了我靠 奖励水平也很稳定 具体日志在
Mamul_Single_Vertical/single_Env_test4.6.3_env1_20251005_014116
长时间训练后，依旧存在些许性能下降问题。需要早停等优化手段。

#### Release 4.6.3.1
加柜子之后，原有的一米范围内接近奖励有问题，车辆也容易被柜子卡主。critic、actor都无法收敛。
修改了接近奖励的范围到终点后方长1.5m，宽1m的长方形区域内
修改障碍区域外的靠墙惩罚起点，1.3开始
修改目标点附近的角度奖励，在接近区域内使用原有函数，其他区域内下限提高，上限降低。
添加正对奖励，最后停止时面向正前方有奖励
前方障碍物惩罚增加，距离变长到0.8m
single_Env_test4.6.3.1_warehouse2_end2_normal_20251005_160520 **废弃**
表现：效果很差，估计不能这样加，因为卸货点附近没有障碍物，这样根据特化奖励，得把两个分开训练。

#### Release 4.6.3.2 废弃
在1的基础上，暂时不前往卸货点，只前往取货点。
修改了角度奖励，在接近区域给予额外的奖励
修改了环境
single_Env_test4.6.3.2_warehouse2_end2_normal_20251005_173805 使用环境2 **修改环境无用**
single_Env_test4.6.3.2_warehouse2_end2_normal_20251005_164339 使用环境1 **接近率提高，完成率降低 不是奖励修改带来的加成，是任务变简单带来的加成**

#### Release 4.6.3.3 废弃
在2的基础上测试降低训练频率增加策略延迟
single_Env_test4.6.3.3_warehouse2_end2_normal_20251005_174846 **废弃**

#### Release 4.6.3.4 废弃
在2的基础上,减小整个靠近惩罚系数 5->2，现在穿过障碍物也有困难；
降低角度移动系数 50->40 ，提高接近奖励 *2  和基础距离奖励 0.4->0.6 降低附近角速度惩罚 *0.5 
single_R4.6.3.4_w2_end2_Nor_20251005_182849 使用环境2 **废弃**

#### Release 4.6.3.5 废弃
在4的基础上，测试随机障碍物  **似乎有一定作用。可以尝试更大范围的渐进学习**
single_R4.6.3.5_w2_end2_Nor_20251005_183222

#### Release 4.6.3.6 废弃
在4的基础上，测试随机障碍物 200万步 single_R4.6.3.6_w2_end2_Nor_20251005_213149 **障碍物变多后就没用了**

#### Release 4.6.3.7 废弃
在4.3.6.2的基础上，同时前往两种点 single_R4.6.3.7_W2_E1_normal_20251005_234556

#### Release 4.6.3.8 废弃
在4.3.6.0基础上把目标点外移，修改环境向右侧偏移障碍物分布，同时修改函数关于距离的对称轴 
现在前后障碍物距离目标点很近了其实，需要进一步修改奖励

#### Release 4.6.3.9
在4.3.6.0基础上，修改了奖励函数，修正了靠近墙壁惩罚
1. 奖励较小的时候也会到达，奖励增大却不会，训练曲线有问题 --降低基础奖惩值
2. 碰撞惩罚有问题，似乎不会有很明显的惩罚

#### Release 4.6.3.9
在4.3.6.0基础上，修改了奖励函数，修正了靠近墙壁惩罚，测试新的随机障碍物
在新环境上效果不错，但是出现较为严重的策略退化，全部退化回最初始的策略。
之后修改了基础奖励到0.7，有一定的稳定训练的效果
##### Release 4.6.3.9 T5



#### Release 4.6.10
基于 single_R4.6.3.9_W2E3_Nor_T5.7
1. 使用0.5到达阈值
2. 增大到达奖励到4000
3. 使用小课表
4. 阶段锁定，有利于背地图

还需测试：
1. 测试0.3阈值
2. 测试其他货物类型

T0：再次训练已有的，加入平均加速度等做对比

T1: 0.3阈值 single_R4.6.10_W2E4_Nor_T1_20251008_163225 **success**
T1.1：再次训练已有的，加入平均加速度等做对比

T2: 易碎货物类型
T2.1：重新训练，加入其他指标
T2.2：放宽阈值，与T3做出区分

T3：危险货物类型
T3.1：危险货物类型,把易碎品损失也添加进来
T3.2：危险货物类型，进一步给予障碍惩罚 **Failed**
T3.3: 危险货物类型，在3.1版本基础上修改奖励 训练16万步 **success**
T3.4: T3.3版本基础上，修改阈值，延长课程 **success** **最终使用版本**

T4：在已有模型基础上训练易碎

T5：在已有模型基础上训练危险

T6: 重新训练基础模型，把危险模型等出发点也加入。
T6.1: T6版本基础上，延长课程 
single_R4.6.10_W2E4_T6.1_normal_20251010_000759 
single_R4.6.10_W2E4_T6.1_normal_20251009_221340

T6：42000课表
T7：2000到达奖励
